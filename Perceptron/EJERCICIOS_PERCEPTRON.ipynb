{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhjJmvAf07sNsDI7BH84aD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnaKarenDRIV/FinanzasUniversitarias/blob/main/Perceptron/EJERCICIOS_PERCEPTRON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EJERCICIOS PERCEPTRÓN"
      ],
      "metadata": {
        "id": "j3lWif4LIsox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DEL PROFESOR:"
      ],
      "metadata": {
        "id": "JEh8sVbGL2h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Entradas para el perceptron\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "# Salidas\n",
        "Y = np.array([0, 0, 0, 1])\n",
        "# Pesos para las entradas\n",
        "weights = np.array([1.0, 0.5])\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.01\n",
        "# Epocas\n",
        "epochs = 100\n",
        "# Sesgo\n",
        "bias =  1.0\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "            Constructor del perceptron:\n",
        "            Guarda las variables\n",
        "            lr -> tasa de aprendizaje\n",
        "            epochs -> numero de epocas\n",
        "            weights -> vector de pesos iniciales\n",
        "            bias -> sesgo inicial\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "            Realiza el entrenamiento del Perceptron.\n",
        "        \"\"\"\n",
        "        # Recorre el dataset la cantidad indicada en epocas\n",
        "        for epoch in range(self.epochs):\n",
        "            for j in range(X.shape[0]):\n",
        "                # Calcula la salida del perceptrón para la entrada actual\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred\n",
        "                # Actualiza los pesos y el sesgo\n",
        "                self.weights += self.lr * loss * X[j]\n",
        "                self.bias += self.lr * loss\n",
        "            print(f\"Epoch {epoch}, Optimized Weights are {self.weights}, and bias is {self.bias}\")\n",
        "        # Imprime los valores finales de los parámetros aprendidos\n",
        "        print(f\"Optimized Weights are {self.weights} and bias is {self.bias}\")\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "            Función de activacion escalon\n",
        "        \"\"\"\n",
        "        return 1 if activation >= 0 else 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "            Calcula la salida del Perceptron para cada fila de entradas X.\n",
        "        \"\"\"\n",
        "        # Calcula producto punto + bias para todas las entradas\n",
        "        sum_ = np.dot(X, self.weights) + self.bias\n",
        "        # Mensaje input y su predicción\n",
        "        for i, s in enumerate(sum_):\n",
        "            print(f\"Input: {X[i]}, Predictions: {self.activation_function(sum_[i])}\")\n",
        "        # Devuelve un array con todas las predicciones\n",
        "        return np.array([self.activation_function(s) for s in sum_])\n",
        "\n",
        "# Crear una instancia del perceptrón\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)\n",
        "# Usar el modelo entrenado para realizar predicciones\n",
        "predictions = p.prediction(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1nE3vU3L7A5",
        "outputId": "d58f253c-cfa4-4d8b-f724-111df4384026"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Optimized Weights are [0.99 0.49], and bias is 0.97\n",
            "Epoch 1, Optimized Weights are [0.98 0.48], and bias is 0.94\n",
            "Epoch 2, Optimized Weights are [0.97 0.47], and bias is 0.9099999999999999\n",
            "Epoch 3, Optimized Weights are [0.96 0.46], and bias is 0.8799999999999999\n",
            "Epoch 4, Optimized Weights are [0.95 0.45], and bias is 0.8499999999999999\n",
            "Epoch 5, Optimized Weights are [0.94 0.44], and bias is 0.8199999999999998\n",
            "Epoch 6, Optimized Weights are [0.93 0.43], and bias is 0.7899999999999998\n",
            "Epoch 7, Optimized Weights are [0.92 0.42], and bias is 0.7599999999999998\n",
            "Epoch 8, Optimized Weights are [0.91 0.41], and bias is 0.7299999999999998\n",
            "Epoch 9, Optimized Weights are [0.9 0.4], and bias is 0.6999999999999997\n",
            "Epoch 10, Optimized Weights are [0.89 0.39], and bias is 0.6699999999999997\n",
            "Epoch 11, Optimized Weights are [0.88 0.38], and bias is 0.6399999999999997\n",
            "Epoch 12, Optimized Weights are [0.87 0.37], and bias is 0.6099999999999997\n",
            "Epoch 13, Optimized Weights are [0.86 0.36], and bias is 0.5799999999999996\n",
            "Epoch 14, Optimized Weights are [0.85 0.35], and bias is 0.5499999999999996\n",
            "Epoch 15, Optimized Weights are [0.84 0.34], and bias is 0.5199999999999996\n",
            "Epoch 16, Optimized Weights are [0.83 0.33], and bias is 0.48999999999999955\n",
            "Epoch 17, Optimized Weights are [0.82 0.32], and bias is 0.4599999999999995\n",
            "Epoch 18, Optimized Weights are [0.81 0.31], and bias is 0.4299999999999995\n",
            "Epoch 19, Optimized Weights are [0.8 0.3], and bias is 0.39999999999999947\n",
            "Epoch 20, Optimized Weights are [0.79 0.29], and bias is 0.36999999999999944\n",
            "Epoch 21, Optimized Weights are [0.78 0.28], and bias is 0.3399999999999994\n",
            "Epoch 22, Optimized Weights are [0.77 0.27], and bias is 0.3099999999999994\n",
            "Epoch 23, Optimized Weights are [0.76 0.26], and bias is 0.27999999999999936\n",
            "Epoch 24, Optimized Weights are [0.75 0.25], and bias is 0.24999999999999933\n",
            "Epoch 25, Optimized Weights are [0.74 0.24], and bias is 0.2199999999999993\n",
            "Epoch 26, Optimized Weights are [0.73 0.23], and bias is 0.18999999999999928\n",
            "Epoch 27, Optimized Weights are [0.72 0.22], and bias is 0.15999999999999925\n",
            "Epoch 28, Optimized Weights are [0.71 0.21], and bias is 0.12999999999999923\n",
            "Epoch 29, Optimized Weights are [0.7 0.2], and bias is 0.09999999999999924\n",
            "Epoch 30, Optimized Weights are [0.69 0.19], and bias is 0.06999999999999926\n",
            "Epoch 31, Optimized Weights are [0.68 0.18], and bias is 0.03999999999999925\n",
            "Epoch 32, Optimized Weights are [0.67 0.17], and bias is 0.009999999999999247\n",
            "Epoch 33, Optimized Weights are [0.66 0.16], and bias is -0.020000000000000753\n",
            "Epoch 34, Optimized Weights are [0.65 0.15], and bias is -0.04000000000000076\n",
            "Epoch 35, Optimized Weights are [0.64 0.14], and bias is -0.06000000000000076\n",
            "Epoch 36, Optimized Weights are [0.63 0.13], and bias is -0.08000000000000075\n",
            "Epoch 37, Optimized Weights are [0.62 0.12], and bias is -0.10000000000000074\n",
            "Epoch 38, Optimized Weights are [0.61 0.11], and bias is -0.12000000000000073\n",
            "Epoch 39, Optimized Weights are [0.6  0.11], and bias is -0.13000000000000073\n",
            "Epoch 40, Optimized Weights are [0.59 0.11], and bias is -0.14000000000000073\n",
            "Epoch 41, Optimized Weights are [0.58 0.11], and bias is -0.15000000000000074\n",
            "Epoch 42, Optimized Weights are [0.57 0.11], and bias is -0.16000000000000075\n",
            "Epoch 43, Optimized Weights are [0.56 0.11], and bias is -0.17000000000000076\n",
            "Epoch 44, Optimized Weights are [0.55 0.11], and bias is -0.18000000000000077\n",
            "Epoch 45, Optimized Weights are [0.54 0.11], and bias is -0.19000000000000078\n",
            "Epoch 46, Optimized Weights are [0.53 0.11], and bias is -0.2000000000000008\n",
            "Epoch 47, Optimized Weights are [0.52 0.11], and bias is -0.2100000000000008\n",
            "Epoch 48, Optimized Weights are [0.51 0.11], and bias is -0.2200000000000008\n",
            "Epoch 49, Optimized Weights are [0.5  0.11], and bias is -0.23000000000000081\n",
            "Epoch 50, Optimized Weights are [0.49 0.11], and bias is -0.24000000000000082\n",
            "Epoch 51, Optimized Weights are [0.48 0.11], and bias is -0.25000000000000083\n",
            "Epoch 52, Optimized Weights are [0.47 0.11], and bias is -0.26000000000000084\n",
            "Epoch 53, Optimized Weights are [0.46 0.11], and bias is -0.27000000000000085\n",
            "Epoch 54, Optimized Weights are [0.45 0.11], and bias is -0.28000000000000086\n",
            "Epoch 55, Optimized Weights are [0.44 0.11], and bias is -0.29000000000000087\n",
            "Epoch 56, Optimized Weights are [0.43 0.11], and bias is -0.3000000000000009\n",
            "Epoch 57, Optimized Weights are [0.42 0.11], and bias is -0.3100000000000009\n",
            "Epoch 58, Optimized Weights are [0.41 0.11], and bias is -0.3200000000000009\n",
            "Epoch 59, Optimized Weights are [0.4  0.11], and bias is -0.3300000000000009\n",
            "Epoch 60, Optimized Weights are [0.39 0.11], and bias is -0.3400000000000009\n",
            "Epoch 61, Optimized Weights are [0.38 0.11], and bias is -0.3500000000000009\n",
            "Epoch 62, Optimized Weights are [0.37 0.11], and bias is -0.36000000000000093\n",
            "Epoch 63, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 64, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 65, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 66, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 67, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 68, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 69, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 70, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 71, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 72, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 73, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 74, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 75, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 76, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 77, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 78, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 79, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 80, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 81, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 82, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 83, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 84, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 85, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 86, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 87, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 88, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 89, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 90, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 91, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 92, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 93, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 94, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 95, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 96, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 97, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 98, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Epoch 99, Optimized Weights are [0.36 0.11], and bias is -0.37000000000000094\n",
            "Optimized Weights are [0.36 0.11] and bias is -0.37000000000000094\n",
            "Input: [0 0], Predictions: 0\n",
            "Input: [0 1], Predictions: 0\n",
            "Input: [1 0], Predictions: 0\n",
            "Input: [1 1], Predictions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##1.-Para el ejemplo Operación lógica AND, cambie las variables épocas, sesgo, pesos y función de activación y observe que pasa."
      ],
      "metadata": {
        "id": "-qoSg9xzJqHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXdc0-6KHmes",
        "outputId": "35995c7f-e105-4c74-c4f1-3ddb1c6d637f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Pesos: [0.99 0.49], Bias: 0.97\n",
            "Epoch 1, Pesos: [0.98 0.48], Bias: 0.94\n",
            "Epoch 2, Pesos: [0.97 0.47], Bias: 0.9099999999999999\n",
            "Epoch 3, Pesos: [0.96 0.46], Bias: 0.8799999999999999\n",
            "Epoch 4, Pesos: [0.95 0.45], Bias: 0.8499999999999999\n",
            "Epoch 5, Pesos: [0.94 0.44], Bias: 0.8199999999999998\n",
            "Epoch 6, Pesos: [0.93 0.43], Bias: 0.7899999999999998\n",
            "Epoch 7, Pesos: [0.92 0.42], Bias: 0.7599999999999998\n",
            "Epoch 8, Pesos: [0.91 0.41], Bias: 0.7299999999999998\n",
            "Epoch 9, Pesos: [0.9 0.4], Bias: 0.6999999999999997\n",
            "Epoch 10, Pesos: [0.89 0.39], Bias: 0.6699999999999997\n",
            "Epoch 11, Pesos: [0.88 0.38], Bias: 0.6399999999999997\n",
            "Epoch 12, Pesos: [0.87 0.37], Bias: 0.6099999999999997\n",
            "Epoch 13, Pesos: [0.86 0.36], Bias: 0.5799999999999996\n",
            "Epoch 14, Pesos: [0.85 0.35], Bias: 0.5499999999999996\n",
            "Epoch 15, Pesos: [0.84 0.34], Bias: 0.5199999999999996\n",
            "Epoch 16, Pesos: [0.83 0.33], Bias: 0.48999999999999955\n",
            "Epoch 17, Pesos: [0.82 0.32], Bias: 0.4599999999999995\n",
            "Epoch 18, Pesos: [0.81 0.31], Bias: 0.4299999999999995\n",
            "Epoch 19, Pesos: [0.8 0.3], Bias: 0.39999999999999947\n",
            "Epoch 20, Pesos: [0.79 0.29], Bias: 0.36999999999999944\n",
            "Epoch 21, Pesos: [0.78 0.28], Bias: 0.3399999999999994\n",
            "Epoch 22, Pesos: [0.77 0.27], Bias: 0.3099999999999994\n",
            "Epoch 23, Pesos: [0.76 0.26], Bias: 0.27999999999999936\n",
            "Epoch 24, Pesos: [0.75 0.25], Bias: 0.24999999999999933\n",
            "Epoch 25, Pesos: [0.74 0.24], Bias: 0.2199999999999993\n",
            "Epoch 26, Pesos: [0.73 0.23], Bias: 0.18999999999999928\n",
            "Epoch 27, Pesos: [0.72 0.22], Bias: 0.15999999999999925\n",
            "Epoch 28, Pesos: [0.71 0.21], Bias: 0.12999999999999923\n",
            "Epoch 29, Pesos: [0.7 0.2], Bias: 0.09999999999999924\n",
            "Epoch 30, Pesos: [0.69 0.19], Bias: 0.06999999999999926\n",
            "Epoch 31, Pesos: [0.68 0.18], Bias: 0.03999999999999925\n",
            "Epoch 32, Pesos: [0.67 0.17], Bias: 0.009999999999999247\n",
            "Epoch 33, Pesos: [0.66 0.16], Bias: -0.020000000000000753\n",
            "Epoch 34, Pesos: [0.65 0.15], Bias: -0.04000000000000076\n",
            "Epoch 35, Pesos: [0.64 0.14], Bias: -0.06000000000000076\n",
            "Epoch 36, Pesos: [0.63 0.13], Bias: -0.08000000000000075\n",
            "Epoch 37, Pesos: [0.62 0.12], Bias: -0.10000000000000074\n",
            "Epoch 38, Pesos: [0.61 0.11], Bias: -0.12000000000000073\n",
            "Epoch 39, Pesos: [0.6  0.11], Bias: -0.13000000000000073\n",
            "Epoch 40, Pesos: [0.59 0.11], Bias: -0.14000000000000073\n",
            "Epoch 41, Pesos: [0.58 0.11], Bias: -0.15000000000000074\n",
            "Epoch 42, Pesos: [0.57 0.11], Bias: -0.16000000000000075\n",
            "Epoch 43, Pesos: [0.56 0.11], Bias: -0.17000000000000076\n",
            "Epoch 44, Pesos: [0.55 0.11], Bias: -0.18000000000000077\n",
            "Epoch 45, Pesos: [0.54 0.11], Bias: -0.19000000000000078\n",
            "Epoch 46, Pesos: [0.53 0.11], Bias: -0.2000000000000008\n",
            "Epoch 47, Pesos: [0.52 0.11], Bias: -0.2100000000000008\n",
            "Epoch 48, Pesos: [0.51 0.11], Bias: -0.2200000000000008\n",
            "Epoch 49, Pesos: [0.5  0.11], Bias: -0.23000000000000081\n",
            "Epoch 50, Pesos: [0.49 0.11], Bias: -0.24000000000000082\n",
            "Epoch 51, Pesos: [0.48 0.11], Bias: -0.25000000000000083\n",
            "Epoch 52, Pesos: [0.47 0.11], Bias: -0.26000000000000084\n",
            "Epoch 53, Pesos: [0.46 0.11], Bias: -0.27000000000000085\n",
            "Epoch 54, Pesos: [0.45 0.11], Bias: -0.28000000000000086\n",
            "Epoch 55, Pesos: [0.44 0.11], Bias: -0.29000000000000087\n",
            "Epoch 56, Pesos: [0.43 0.11], Bias: -0.3000000000000009\n",
            "Epoch 57, Pesos: [0.42 0.11], Bias: -0.3100000000000009\n",
            "Epoch 58, Pesos: [0.41 0.11], Bias: -0.3200000000000009\n",
            "Epoch 59, Pesos: [0.4  0.11], Bias: -0.3300000000000009\n",
            "Epoch 60, Pesos: [0.39 0.11], Bias: -0.3400000000000009\n",
            "Epoch 61, Pesos: [0.38 0.11], Bias: -0.3500000000000009\n",
            "Epoch 62, Pesos: [0.37 0.11], Bias: -0.36000000000000093\n",
            "Epoch 63, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 64, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 65, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 66, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 67, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 68, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 69, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 70, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 71, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 72, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 73, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 74, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 75, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 76, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 77, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 78, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 79, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 80, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 81, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 82, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 83, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 84, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 85, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 86, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 87, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 88, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 89, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 90, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 91, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 92, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 93, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 94, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 95, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 96, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 97, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 98, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Epoch 99, Pesos: [0.36 0.11], Bias: -0.37000000000000094\n",
            "Pesos finales: [0.36 0.11], Bias final: -0.37000000000000094\n",
            "Input: [0 0], Prediction: 0\n",
            "Input: [0 1], Prediction: 0\n",
            "Input: [1 0], Prediction: 0\n",
            "Input: [1 1], Prediction: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np                                                                          # 1) Importamos numpy para manejar operaciones matemáticas con vectores y matrices.\n",
        "\n",
        "# Entradas para el perceptrón (todas las combinaciones de A y B)\n",
        "X = np.array([[0, 0],                                                                       # 2) Entrada 1: A=0, B=0\n",
        "              [0, 1],                                                                       # 3) Entrada 2: A=0, B=1\n",
        "              [1, 0],                                                                       # 4) Entrada 3: A=1, B=0\n",
        "              [1, 1]])                                                                      # 5) Entrada 4: A=1, B=1\n",
        "\n",
        "# Salidas esperadas (operación AND)\n",
        "Y = np.array([0, 0, 0, 1])                                                                  # 6) Solo la combinación [1,1] produce salida 1 en el AND.\n",
        "\n",
        "# Pesos iniciales (valores con los que empieza el entrenamiento)\n",
        "weights = np.array([1.0, 0.5])                                                              # 7) Pesos iniciales de las conexiones A y B con la neurona.\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.01                                                                                   # 8) Define qué tan rápido se ajustan los pesos en cada iteración.\n",
        "\n",
        "# Número de épocas (vueltas completas al conjunto de datos)\n",
        "epochs = 100                                                                                # 9) Cuántas veces se repetirá el entrenamiento.\n",
        "\n",
        "# Sesgo (bias)\n",
        "bias = 1.0                                                                                  # 10) Valor inicial del sesgo, que actúa como un desplazamiento en la activación.\n",
        "\n",
        "# Definición de la clase Perceptron\n",
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "        Constructor del perceptrón:\n",
        "        Guarda las variables:\n",
        "        lr -> tasa de aprendizaje\n",
        "        epochs -> número de épocas\n",
        "        weights -> vector de pesos iniciales\n",
        "        bias -> sesgo inicial\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Entrena el perceptrón ajustando pesos y sesgo.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):                                                    # 11) Bucle principal: recorre todas las épocas de entrenamiento.\n",
        "            for j in range(X.shape[0]):                                                     # 12) Itera sobre cada fila de entradas (cada patrón A,B).\n",
        "                y_pred = self.activation_function(                                          # 13) Calcula la salida del perceptrón.\n",
        "                    np.dot(self.weights, X[j]) + self.bias\n",
        "                )\n",
        "                loss = Y[j] - y_pred                                                        # 14) Calcula el error: salida deseada - salida predicha.\n",
        "                self.weights += self.lr * loss * X[j]                                       # 15) Ajusta los pesos según el error.\n",
        "                self.bias += self.lr * loss                                                 # 16) Ajusta el sesgo en función del error.\n",
        "            print(f\"Epoch {epoch}, Pesos: {self.weights}, Bias: {self.bias}\")               # 17) Muestra cómo cambian los valores en cada época.\n",
        "        print(f\"Pesos finales: {self.weights}, Bias final: {self.bias}\")                    # 18) Muestra los valores optimizados finales.\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "        Función de activación tipo escalón.\n",
        "        Si el resultado es >= 0, devuelve 1 (activo); si no, devuelve 0.\n",
        "        \"\"\"\n",
        "        return 1 if activation >= 0 else 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "        Calcula la salida del perceptrón ya entrenado para cada entrada X.\n",
        "        \"\"\"\n",
        "        sum_ = np.dot(X, self.weights) + self.bias                                           # 19) Calcula el producto punto + bias.\n",
        "        for i, s in enumerate(sum_):                                                         # 20) Recorre las entradas para mostrar predicciones.\n",
        "            print(f\"Input: {X[i]}, Prediction: {self.activation_function(s)}\")\n",
        "        return np.array([self.activation_function(s) for s in sum_])                         # 21) Devuelve las salidas como arreglo.\n",
        "\n",
        "# Crear una instancia del perceptrón con los valores definidos arriba\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)                                                                                  # 22) Inicia el proceso de entrenamiento.\n",
        "\n",
        "# Usar el modelo entrenado para hacer predicciones\n",
        "predictions = p.prediction(X)                                                                # 23) Calcula las salidas después del entrenamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##2.-Resuelva el problema para la operacion lógica OR, utilizando la mism clase utilizada en el ejemplo Operación lógica AND"
      ],
      "metadata": {
        "id": "ow0kRwQQPIrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                                                                      # 1) Importamos numpy para manejar vectores y matrices.\n",
        "\n",
        "# Entradas para el perceptrón (todas las combinaciones de A y B)\n",
        "X = np.array([[0, 0],                                                                   # 2) Entrada 1: A=0, B=0\n",
        "              [0, 1],                                                                   # 3) Entrada 2: A=0, B=1\n",
        "              [1, 0],                                                                   # 4) Entrada 3: A=1, B=0\n",
        "              [1, 1]])                                                                  # 5) Entrada 4: A=1, B=1\n",
        "\n",
        "# Salidas esperadas (operación OR)\n",
        "Y = np.array([0, 1, 1, 1])                                                              # 6) En la operación OR, la salida es 1 si al menos una entrada es 1.\n",
        "\n",
        "# Pesos iniciales\n",
        "weights = np.array([0.5, 0.5])                                                          # 7) Valores iniciales de los pesos. Se pueden ajustar durante el entrenamiento.\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.1                                                                                # 8) Tasa de aprendizaje más alta para que los pesos se ajusten más rápido.\n",
        "\n",
        "# Número de épocas (vueltas de entrenamiento)\n",
        "epochs = 20                                                                             # 9) Número de veces que el perceptrón recorrerá todas las entradas.\n",
        "\n",
        "# Sesgo (bias)\n",
        "bias = -0.5                                                                             # 10) Valor inicial del sesgo. Para OR, un sesgo negativo permite activarse con suma >= 0.\n",
        "\n",
        "# Definición de la clase Perceptron (igual a la del ejemplo de AND)\n",
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Entrena el perceptrón ajustando pesos y sesgo.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            for j in range(X.shape[0]):\n",
        "                # Calcula la salida actual\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred\n",
        "                # Actualiza los pesos y el sesgo\n",
        "                self.weights += self.lr * loss * X[j]\n",
        "                self.bias += self.lr * loss\n",
        "            # Imprime los resultados de cada época\n",
        "            print(f\"Época {epoch}: Pesos = {self.weights}, Bias = {self.bias}\")\n",
        "        print(f\"\\nPesos finales: {self.weights}, Bias final: {self.bias}\")\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "        Función de activación tipo escalón (step function).\n",
        "        \"\"\"\n",
        "        return 1 if activation >= 0 else 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "        Calcula la salida del perceptrón ya entrenado.\n",
        "        \"\"\"\n",
        "        sum_ = np.dot(X, self.weights) + self.bias\n",
        "        for i, s in enumerate(sum_):\n",
        "            print(f\"Entrada: {X[i]}, Salida predicha: {self.activation_function(s)}\")\n",
        "        return np.array([self.activation_function(s) for s in sum_])\n",
        "\n",
        "# Crear una instancia del perceptrón para OR\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)\n",
        "\n",
        "# Realizar las predicciones finales\n",
        "predictions = p.prediction(X)\n",
        "\n",
        "# Mostrar resultados finales\n",
        "print(\"\\nEntradas:\\n\", X)\n",
        "print(\"Salidas esperadas (OR):\", Y)\n",
        "print(\"Salidas predichas:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsqwZ5n_PKsR",
        "outputId": "947ec004-0524-47a1-dd56-2498872df8b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 0: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 1: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 2: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 3: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 4: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 5: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 6: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 7: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 8: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 9: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 10: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 11: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 12: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 13: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 14: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 15: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 16: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 17: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 18: Pesos = [0.5 0.5], Bias = -0.5\n",
            "Época 19: Pesos = [0.5 0.5], Bias = -0.5\n",
            "\n",
            "Pesos finales: [0.5 0.5], Bias final: -0.5\n",
            "Entrada: [0 0], Salida predicha: 0\n",
            "Entrada: [0 1], Salida predicha: 1\n",
            "Entrada: [1 0], Salida predicha: 1\n",
            "Entrada: [1 1], Salida predicha: 1\n",
            "\n",
            "Entradas:\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Salidas esperadas (OR): [0 1 1 1]\n",
            "Salidas predichas: [0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.-Intente usar el Perceptron para resolver una operacion matematica simple, similar a lo mostrado en Doble de un número."
      ],
      "metadata": {
        "id": "LUwAVngySxCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PARA EL TERCER EJERCICIO TENEMOS QUE TOMAR EN CUENTA EL EJERCICIO DEL PROFESOR"
      ],
      "metadata": {
        "id": "YYsEB_MDR5H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODIGO DEL PROFESOR"
      ],
      "metadata": {
        "id": "j_EQprYeSaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Entradas para el perceptron\n",
        "X = np.array([[1],\n",
        "              [2],\n",
        "              [3],\n",
        "              [4]])\n",
        "# Salidas\n",
        "Y = np.array([2, 4, 6, 8])\n",
        "# Pesos para las entradas\n",
        "weights = np.array([0.5])\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.1\n",
        "# Epocas\n",
        "epochs = 100\n",
        "# Sesgo\n",
        "bias =  0.5\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "            Constructor del perceptron:\n",
        "            Guarda las variables\n",
        "            lr -> tasa de aprendizaje\n",
        "            epochs -> numero de epocas\n",
        "            weights -> vector de pesos iniciales\n",
        "            bias -> sesgo inicial\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "            Realiza el entrenamiento del Perceptron.\n",
        "        \"\"\"\n",
        "        # Recorre el dataset la cantidad indicada en epocas\n",
        "        for epoch in range(self.epochs):\n",
        "            for j in range(X.shape[0]):\n",
        "                # Calcula la salida del perceptrón para la entrada actual\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred\n",
        "                # Actualiza los pesos y el sesgo\n",
        "                self.weights += self.lr * loss * X[j]\n",
        "                self.bias += self.lr * loss\n",
        "            print(f\"Epoch {epoch}, Optimized Weights are {self.weights}, and bias is {self.bias}\")\n",
        "        # Imprime los valores finales de los parámetros aprendidos\n",
        "        print(f\"Optimized Weights are {self.weights} and bias is {self.bias}\")\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "            Función de activacion ReLU\n",
        "        \"\"\"\n",
        "        return np.maximum(0, activation)\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "            Calcula la salida del Perceptron para cada fila de entradas X.\n",
        "        \"\"\"\n",
        "        # Calcula producto punto + bias para todas las entradas\n",
        "        sum_ = np.dot(X, self.weights) + self.bias\n",
        "        # Mensaje input y su predicción\n",
        "        for i, s in enumerate(sum_):\n",
        "            print(f\"Input: {X[i]}, Predictions: {self.activation_function(sum_[i])}\")\n",
        "        # Devuelve un array con todas las predicciones\n",
        "        return np.array([self.activation_function(s) for s in sum_])\n",
        "\n",
        "# Crear una instancia del perceptrón\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)\n",
        "# Usar el modelo entrenado para realizar predicciones\n",
        "predictions = p.prediction(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333jZwI7SnLl",
        "outputId": "283fe9f8-f36a-43f0-9c3b-1f09638e5287"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Optimized Weights are [1.7948], and bias is 1.0602\n",
            "Epoch 1, Optimized Weights are [1.8094718], and bias is 0.9843957\n",
            "Epoch 2, Optimized Weights are [1.82309457], and bias is 0.9140114074499999\n",
            "Epoch 3, Optimized Weights are [1.8357433], and bias is 0.8486595918173249\n",
            "Epoch 4, Optimized Weights are [1.84748766], and bias is 0.7879804310023861\n",
            "Epoch 5, Optimized Weights are [1.85839229], and bias is 0.7316398301857157\n",
            "Epoch 6, Optimized Weights are [1.86851724], and bias is 0.6793275823274367\n",
            "Epoch 7, Optimized Weights are [1.87791826], and bias is 0.630755660191025\n",
            "Epoch 8, Optimized Weights are [1.8866471], and bias is 0.5856566304873667\n",
            "Epoch 9, Optimized Weights are [1.89475184], and bias is 0.5437821814075201\n",
            "Epoch 10, Optimized Weights are [1.90227708], and bias is 0.5049017554368825\n",
            "Epoch 11, Optimized Weights are [1.90926427], and bias is 0.4688012799231454\n",
            "Epoch 12, Optimized Weights are [1.91575187], and bias is 0.4352819884086404\n",
            "Epoch 13, Optimized Weights are [1.92177561], and bias is 0.4041593262374226\n",
            "Epoch 14, Optimized Weights are [1.92736866], and bias is 0.3752619344114469\n",
            "Epoch 15, Optimized Weights are [1.9325618], and bias is 0.3484307061010285\n",
            "Epoch 16, Optimized Weights are [1.93738363], and bias is 0.3235179106148049\n",
            "Epoch 17, Optimized Weights are [1.9418607], and bias is 0.3003863800058463\n",
            "Epoch 18, Optimized Weights are [1.94601766], and bias is 0.2789087538354283\n",
            "Epoch 19, Optimized Weights are [1.9498774], and bias is 0.2589667779361951\n",
            "Epoch 20, Optimized Weights are [1.95346116], and bias is 0.24045065331375723\n",
            "Epoch 21, Optimized Weights are [1.95678869], and bias is 0.22325843160182351\n",
            "Epoch 22, Optimized Weights are [1.9598783], and bias is 0.20729545374229322\n",
            "Epoch 23, Optimized Weights are [1.962747], and bias is 0.1924738287997191\n",
            "Epoch 24, Optimized Weights are [1.96541059], and bias is 0.17871195004053916\n",
            "Epoch 25, Optimized Weights are [1.96788373], and bias is 0.16593404561264077\n",
            "Epoch 26, Optimized Weights are [1.97018005], and bias is 0.15406976135133696\n",
            "Epoch 27, Optimized Weights are [1.97231217], and bias is 0.14305377341471626\n",
            "Epoch 28, Optimized Weights are [1.97429185], and bias is 0.13282542861556412\n",
            "Epoch 29, Optimized Weights are [1.97612999], and bias is 0.12332841046955126\n",
            "Epoch 30, Optimized Weights are [1.97783669], and bias is 0.11451042912097835\n",
            "Epoch 31, Optimized Weights are [1.97942137], and bias is 0.10632293343882823\n",
            "Epoch 32, Optimized Weights are [1.98089274], and bias is 0.09872084369795207\n",
            "Epoch 33, Optimized Weights are [1.98225891], and bias is 0.09166230337354858\n",
            "Epoch 34, Optimized Weights are [1.9835274], and bias is 0.08510844868233984\n",
            "Epoch 35, Optimized Weights are [1.98470519], and bias is 0.07902319460155247\n",
            "Epoch 36, Optimized Weights are [1.98579877], and bias is 0.07337303618754157\n",
            "Epoch 37, Optimized Weights are [1.98681416], and bias is 0.06812686410013225\n",
            "Epoch 38, Optimized Weights are [1.98775694], and bias is 0.06325579331697284\n",
            "Epoch 39, Optimized Weights are [1.98863232], and bias is 0.05873300409480917\n",
            "Epoch 40, Optimized Weights are [1.98944511], and bias is 0.054533594302030246\n",
            "Epoch 41, Optimized Weights are [1.99019979], and bias is 0.05063444230943498\n",
            "Epoch 42, Optimized Weights are [1.9909005], and bias is 0.04701407968431038\n",
            "Epoch 43, Optimized Weights are [1.99155111], and bias is 0.043652572986882236\n",
            "Epoch 44, Optimized Weights are [1.99215521], and bias is 0.040531414018320125\n",
            "Epoch 45, Optimized Weights are [1.99271611], and bias is 0.03763341791601041\n",
            "Epoch 46, Optimized Weights are [1.99323691], and bias is 0.034942628535015625\n",
            "Epoch 47, Optimized Weights are [1.99372047], and bias is 0.032444230594761876\n",
            "Epoch 48, Optimized Weights are [1.99416946], and bias is 0.030124468107236443\n",
            "Epoch 49, Optimized Weights are [1.99458634], and bias is 0.027970568637569002\n",
            "Epoch 50, Optimized Weights are [1.99497342], and bias is 0.025970672979982734\n",
            "Epoch 51, Optimized Weights are [1.99533282], and bias is 0.0241137698619141\n",
            "Epoch 52, Optimized Weights are [1.99566652], and bias is 0.022389635316787136\n",
            "Epoch 53, Optimized Weights are [1.99597637], and bias is 0.020788776391636843\n",
            "Epoch 54, Optimized Weights are [1.99626406], and bias is 0.019302378879634943\n",
            "Epoch 55, Optimized Weights are [1.99653118], and bias is 0.017922258789740936\n",
            "Epoch 56, Optimized Weights are [1.9967792], and bias is 0.016640817286274572\n",
            "Epoch 57, Optimized Weights are [1.99700948], and bias is 0.015450998850305911\n",
            "Epoch 58, Optimized Weights are [1.99722331], and bias is 0.014346252432508838\n",
            "Epoch 59, Optimized Weights are [1.99742184], and bias is 0.013320495383584478\n",
            "Epoch 60, Optimized Weights are [1.99760618], and bias is 0.012368079963658267\n",
            "Epoch 61, Optimized Weights are [1.99777734], and bias is 0.011483762246256572\n",
            "Epoch 62, Optimized Weights are [1.99793626], and bias is 0.010662673245649283\n",
            "Epoch 63, Optimized Weights are [1.99808381], and bias is 0.0099002921085854\n",
            "Epoch 64, Optimized Weights are [1.99822082], and bias is 0.009192421222821661\n",
            "Epoch 65, Optimized Weights are [1.99834803], and bias is 0.008535163105389834\n",
            "Epoch 66, Optimized Weights are [1.99846615], and bias is 0.007924898943354432\n",
            "Epoch 67, Optimized Weights are [1.99857582], and bias is 0.007358268668904669\n",
            "Epoch 68, Optimized Weights are [1.99867765], and bias is 0.006832152459078036\n",
            "Epoch 69, Optimized Weights are [1.9987722], and bias is 0.006343653558253917\n",
            "Epoch 70, Optimized Weights are [1.99885998], and bias is 0.005890082328838734\n",
            "Epoch 71, Optimized Weights are [1.9989415], and bias is 0.005468941442326805\n",
            "Epoch 72, Optimized Weights are [1.99901718], and bias is 0.0050779121292004485\n",
            "Epoch 73, Optimized Weights are [1.99908745], and bias is 0.004714841411962536\n",
            "Epoch 74, Optimized Weights are [1.9991527], and bias is 0.00437773025100722\n",
            "Epoch 75, Optimized Weights are [1.99921328], and bias is 0.004064722538060309\n",
            "Epoch 76, Optimized Weights are [1.99926953], and bias is 0.0037740948765889438\n",
            "Epoch 77, Optimized Weights are [1.99932176], and bias is 0.0035042470929126748\n",
            "Epoch 78, Optimized Weights are [1.99937025], and bias is 0.0032536934257696142\n",
            "Epoch 79, Optimized Weights are [1.99941528], and bias is 0.0030210543458270827\n",
            "Epoch 80, Optimized Weights are [1.99945709], and bias is 0.0028050489601004246\n",
            "Epoch 81, Optimized Weights are [1.99949591], and bias is 0.00260448795945331\n",
            "Epoch 82, Optimized Weights are [1.99953195], and bias is 0.0024182670703523693\n",
            "Epoch 83, Optimized Weights are [1.99956541], and bias is 0.002245360974822191\n",
            "Epoch 84, Optimized Weights are [1.99959649], and bias is 0.0020848176651223124\n",
            "Epoch 85, Optimized Weights are [1.99962534], and bias is 0.0019357532020660866\n",
            "Epoch 86, Optimized Weights are [1.99965213], and bias is 0.0017973468481184247\n",
            "Epoch 87, Optimized Weights are [1.999677], and bias is 0.001668836548478064\n",
            "Epoch 88, Optimized Weights are [1.99970009], and bias is 0.001549514735261875\n",
            "Epoch 89, Optimized Weights are [1.99972154], and bias is 0.0014387244316907016\n",
            "Epoch 90, Optimized Weights are [1.99974145], and bias is 0.001335855634824739\n",
            "Epoch 91, Optimized Weights are [1.99975993], and bias is 0.0012403419569347366\n",
            "Epoch 92, Optimized Weights are [1.9997771], and bias is 0.001151657507013908\n",
            "Epoch 93, Optimized Weights are [1.99979304], and bias is 0.0010693139952622822\n",
            "Epoch 94, Optimized Weights are [1.99980783], and bias is 0.0009928580446010907\n",
            "Epoch 95, Optimized Weights are [1.99982157], and bias is 0.0009218686944121915\n",
            "Epoch 96, Optimized Weights are [1.99983433], and bias is 0.0008559550827616993\n",
            "Epoch 97, Optimized Weights are [1.99984618], and bias is 0.0007947542943442066\n",
            "Epoch 98, Optimized Weights are [1.99985717], and bias is 0.000737929362298544\n",
            "Epoch 99, Optimized Weights are [1.99986739], and bias is 0.0006851674128942766\n",
            "Optimized Weights are [1.99986739] and bias is 0.0006851674128942766\n",
            "Input: [1], Predictions: 2.0005525543652376\n",
            "Input: [2], Predictions: 4.00041994131758\n",
            "Input: [3], Predictions: 6.000287328269923\n",
            "Input: [4], Predictions: 8.000154715222267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESOLUCION DEL EJERCICIO"
      ],
      "metadata": {
        "id": "D743DKcWUeYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones\n",
        "\n",
        "Este perceptrón aprende una función lineal porque ReLU es una función lineal a partir de 0.\n",
        "\n",
        "Cada época ajusta los pesos y el bias para acercar la predicción al doble del número.\n",
        "\n",
        "La línea de depuración (print) permite ver cómo los pesos y el bias cambian en cada época.\n",
        "\n",
        "Al final, las predicciones deberían ser muy cercanas a [2, 4, 6, 8]."
      ],
      "metadata": {
        "id": "QlNEljK1XOL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                                                               # 1) Importamos numpy para manejar arreglos y operaciones matemáticas con vectores.\n",
        "\n",
        "# Entradas (X) y salidas esperadas (Y)\n",
        "X = np.array([[1],                                                               # 2) Primera entrada: 1\n",
        "              [2],                                                               # 3) Segunda entrada: 2\n",
        "              [3],                                                               # 4) Tercera entrada: 3\n",
        "              [4]])                                                              # 5) Cuarta entrada: 4\n",
        "\n",
        "Y = np.array([2, 4, 6, 8])                                                       # 6) Salidas esperadas: el doble de cada entrada.\n",
        "\n",
        "# Parámetros iniciales del perceptrón\n",
        "weights = np.array([0.5])                                                        # 7) Peso inicial para la entrada (se ajustará durante el entrenamiento)\n",
        "lr = 0.1                                                                         # 8) Tasa de aprendizaje: controla cuánto se actualizan los pesos cada vez que hay error\n",
        "epochs = 100                                                                     # 9) Número de iteraciones completas sobre todo el conjunto de datos\n",
        "bias = 0.5                                                                       # 10) Sesgo inicial (bias) que permite desplazar la función de activación hacia arriba o abajo\n",
        "\n",
        "# Definición de la clase Perceptron\n",
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "        Constructor del perceptrón: inicializa todos los parámetros\n",
        "        \"\"\"\n",
        "        self.lr = lr                                                             # 11) Guarda la tasa de aprendizaje\n",
        "        self.epochs = epochs                                                     # 12) Guarda el número de épocas\n",
        "        self.weights = weights                                                   # 13) Guarda los pesos iniciales\n",
        "        self.bias = bias                                                         # 14) Guarda el sesgo inicial\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Entrena el perceptrón ajustando los pesos y el sesgo\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):                                                            # 15) Recorre todas las épocas de entrenamiento\n",
        "            for j in range(X.shape[0]):                                                             # 16) Recorre cada fila de entradas (cada ejemplo de entrenamiento)\n",
        "                # Calcula la salida actual usando la función de activación ReLU\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)           # 17) Producto punto + bias, luego ReLU\n",
        "\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred                                                                # 18) Diferencia entre salida esperada y salida predicha\n",
        "\n",
        "                # Actualiza los pesos\n",
        "                self.weights += self.lr * loss * X[j]                                               # 19) Ajusta los pesos proporcional al error y la entrada\n",
        "\n",
        "                # Actualiza el sesgo\n",
        "                self.bias += self.lr * loss                                                         # 20) Ajusta el bias proporcional al error\n",
        "\n",
        "            # Depuración: mostrar evolución de pesos y bias en cada época\n",
        "            print(f\"Epoch {epoch+1}: weights={self.weights}, bias={self.bias}\")                     # 21) Muestra los pesos y bias después de cada época\n",
        "\n",
        "        # Resultados finales\n",
        "        print(\"\\nEntrenamiento finalizado:\")                                                        # 22) Mensaje indicando fin del entrenamiento\n",
        "        print(f\"Pesos optimizados: {self.weights}\")                                                 # 23) Mostrar pesos finales\n",
        "        print(f\"Sesgo optimizado: {self.bias}\")                                                     # 24) Mostrar bias final\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "        Función de activación ReLU\n",
        "        \"\"\"\n",
        "        return np.maximum(0, activation)                                                            # 25) ReLU: devuelve el valor si es >=0, si es <0 devuelve 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "        Calcula las predicciones del perceptrón\n",
        "        \"\"\"\n",
        "        sum_ = np.dot(X, self.weights) + self.bias                                                 # 26) Calcula la suma ponderada + bias\n",
        "        for i, s in enumerate(sum_):                                                               # 27) Recorre cada entrada y su suma ponderada\n",
        "            print(f\"Input: {X[i]}, Prediction: {self.activation_function(s)}\")                     # 28) Mostrar predicción para cada entrada\n",
        "        return np.array([self.activation_function(s) for s in sum_])                               # 29) Devuelve todas las predicciones como arreglo\n",
        "\n",
        "# Crear e inicializar el perceptrón\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)                                   # 30) Instancia de la clase Perceptron\n",
        "\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)                                                                                        # 31) Entrena el perceptrón con las entradas y salidas definidas\n",
        "\n",
        "# Realizar predicciones finales\n",
        "print(\"\\nPredicciones finales:\")                                                                   # 32) Mensaje de inicio de predicciones\n",
        "predictions = p.prediction(X)                                                                      # 33) Calcula y muestra las predicciones\n",
        "print(\"Predicciones:\", predictions)                                                                # 34) Muestra las predicciones finales en forma de arreglo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCSzZjoaXKUm",
        "outputId": "f0eaf2a7-e9f6-4fc8-b219-c83edf394e8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: weights=[1.7948], bias=1.0602\n",
            "Epoch 2: weights=[1.8094718], bias=0.9843957\n",
            "Epoch 3: weights=[1.82309457], bias=0.9140114074499999\n",
            "Epoch 4: weights=[1.8357433], bias=0.8486595918173249\n",
            "Epoch 5: weights=[1.84748766], bias=0.7879804310023861\n",
            "Epoch 6: weights=[1.85839229], bias=0.7316398301857157\n",
            "Epoch 7: weights=[1.86851724], bias=0.6793275823274367\n",
            "Epoch 8: weights=[1.87791826], bias=0.630755660191025\n",
            "Epoch 9: weights=[1.8866471], bias=0.5856566304873667\n",
            "Epoch 10: weights=[1.89475184], bias=0.5437821814075201\n",
            "Epoch 11: weights=[1.90227708], bias=0.5049017554368825\n",
            "Epoch 12: weights=[1.90926427], bias=0.4688012799231454\n",
            "Epoch 13: weights=[1.91575187], bias=0.4352819884086404\n",
            "Epoch 14: weights=[1.92177561], bias=0.4041593262374226\n",
            "Epoch 15: weights=[1.92736866], bias=0.3752619344114469\n",
            "Epoch 16: weights=[1.9325618], bias=0.3484307061010285\n",
            "Epoch 17: weights=[1.93738363], bias=0.3235179106148049\n",
            "Epoch 18: weights=[1.9418607], bias=0.3003863800058463\n",
            "Epoch 19: weights=[1.94601766], bias=0.2789087538354283\n",
            "Epoch 20: weights=[1.9498774], bias=0.2589667779361951\n",
            "Epoch 21: weights=[1.95346116], bias=0.24045065331375723\n",
            "Epoch 22: weights=[1.95678869], bias=0.22325843160182351\n",
            "Epoch 23: weights=[1.9598783], bias=0.20729545374229322\n",
            "Epoch 24: weights=[1.962747], bias=0.1924738287997191\n",
            "Epoch 25: weights=[1.96541059], bias=0.17871195004053916\n",
            "Epoch 26: weights=[1.96788373], bias=0.16593404561264077\n",
            "Epoch 27: weights=[1.97018005], bias=0.15406976135133696\n",
            "Epoch 28: weights=[1.97231217], bias=0.14305377341471626\n",
            "Epoch 29: weights=[1.97429185], bias=0.13282542861556412\n",
            "Epoch 30: weights=[1.97612999], bias=0.12332841046955126\n",
            "Epoch 31: weights=[1.97783669], bias=0.11451042912097835\n",
            "Epoch 32: weights=[1.97942137], bias=0.10632293343882823\n",
            "Epoch 33: weights=[1.98089274], bias=0.09872084369795207\n",
            "Epoch 34: weights=[1.98225891], bias=0.09166230337354858\n",
            "Epoch 35: weights=[1.9835274], bias=0.08510844868233984\n",
            "Epoch 36: weights=[1.98470519], bias=0.07902319460155247\n",
            "Epoch 37: weights=[1.98579877], bias=0.07337303618754157\n",
            "Epoch 38: weights=[1.98681416], bias=0.06812686410013225\n",
            "Epoch 39: weights=[1.98775694], bias=0.06325579331697284\n",
            "Epoch 40: weights=[1.98863232], bias=0.05873300409480917\n",
            "Epoch 41: weights=[1.98944511], bias=0.054533594302030246\n",
            "Epoch 42: weights=[1.99019979], bias=0.05063444230943498\n",
            "Epoch 43: weights=[1.9909005], bias=0.04701407968431038\n",
            "Epoch 44: weights=[1.99155111], bias=0.043652572986882236\n",
            "Epoch 45: weights=[1.99215521], bias=0.040531414018320125\n",
            "Epoch 46: weights=[1.99271611], bias=0.03763341791601041\n",
            "Epoch 47: weights=[1.99323691], bias=0.034942628535015625\n",
            "Epoch 48: weights=[1.99372047], bias=0.032444230594761876\n",
            "Epoch 49: weights=[1.99416946], bias=0.030124468107236443\n",
            "Epoch 50: weights=[1.99458634], bias=0.027970568637569002\n",
            "Epoch 51: weights=[1.99497342], bias=0.025970672979982734\n",
            "Epoch 52: weights=[1.99533282], bias=0.0241137698619141\n",
            "Epoch 53: weights=[1.99566652], bias=0.022389635316787136\n",
            "Epoch 54: weights=[1.99597637], bias=0.020788776391636843\n",
            "Epoch 55: weights=[1.99626406], bias=0.019302378879634943\n",
            "Epoch 56: weights=[1.99653118], bias=0.017922258789740936\n",
            "Epoch 57: weights=[1.9967792], bias=0.016640817286274572\n",
            "Epoch 58: weights=[1.99700948], bias=0.015450998850305911\n",
            "Epoch 59: weights=[1.99722331], bias=0.014346252432508838\n",
            "Epoch 60: weights=[1.99742184], bias=0.013320495383584478\n",
            "Epoch 61: weights=[1.99760618], bias=0.012368079963658267\n",
            "Epoch 62: weights=[1.99777734], bias=0.011483762246256572\n",
            "Epoch 63: weights=[1.99793626], bias=0.010662673245649283\n",
            "Epoch 64: weights=[1.99808381], bias=0.0099002921085854\n",
            "Epoch 65: weights=[1.99822082], bias=0.009192421222821661\n",
            "Epoch 66: weights=[1.99834803], bias=0.008535163105389834\n",
            "Epoch 67: weights=[1.99846615], bias=0.007924898943354432\n",
            "Epoch 68: weights=[1.99857582], bias=0.007358268668904669\n",
            "Epoch 69: weights=[1.99867765], bias=0.006832152459078036\n",
            "Epoch 70: weights=[1.9987722], bias=0.006343653558253917\n",
            "Epoch 71: weights=[1.99885998], bias=0.005890082328838734\n",
            "Epoch 72: weights=[1.9989415], bias=0.005468941442326805\n",
            "Epoch 73: weights=[1.99901718], bias=0.0050779121292004485\n",
            "Epoch 74: weights=[1.99908745], bias=0.004714841411962536\n",
            "Epoch 75: weights=[1.9991527], bias=0.00437773025100722\n",
            "Epoch 76: weights=[1.99921328], bias=0.004064722538060309\n",
            "Epoch 77: weights=[1.99926953], bias=0.0037740948765889438\n",
            "Epoch 78: weights=[1.99932176], bias=0.0035042470929126748\n",
            "Epoch 79: weights=[1.99937025], bias=0.0032536934257696142\n",
            "Epoch 80: weights=[1.99941528], bias=0.0030210543458270827\n",
            "Epoch 81: weights=[1.99945709], bias=0.0028050489601004246\n",
            "Epoch 82: weights=[1.99949591], bias=0.00260448795945331\n",
            "Epoch 83: weights=[1.99953195], bias=0.0024182670703523693\n",
            "Epoch 84: weights=[1.99956541], bias=0.002245360974822191\n",
            "Epoch 85: weights=[1.99959649], bias=0.0020848176651223124\n",
            "Epoch 86: weights=[1.99962534], bias=0.0019357532020660866\n",
            "Epoch 87: weights=[1.99965213], bias=0.0017973468481184247\n",
            "Epoch 88: weights=[1.999677], bias=0.001668836548478064\n",
            "Epoch 89: weights=[1.99970009], bias=0.001549514735261875\n",
            "Epoch 90: weights=[1.99972154], bias=0.0014387244316907016\n",
            "Epoch 91: weights=[1.99974145], bias=0.001335855634824739\n",
            "Epoch 92: weights=[1.99975993], bias=0.0012403419569347366\n",
            "Epoch 93: weights=[1.9997771], bias=0.001151657507013908\n",
            "Epoch 94: weights=[1.99979304], bias=0.0010693139952622822\n",
            "Epoch 95: weights=[1.99980783], bias=0.0009928580446010907\n",
            "Epoch 96: weights=[1.99982157], bias=0.0009218686944121915\n",
            "Epoch 97: weights=[1.99983433], bias=0.0008559550827616993\n",
            "Epoch 98: weights=[1.99984618], bias=0.0007947542943442066\n",
            "Epoch 99: weights=[1.99985717], bias=0.000737929362298544\n",
            "Epoch 100: weights=[1.99986739], bias=0.0006851674128942766\n",
            "\n",
            "Entrenamiento finalizado:\n",
            "Pesos optimizados: [1.99986739]\n",
            "Sesgo optimizado: 0.0006851674128942766\n",
            "\n",
            "Predicciones finales:\n",
            "Input: [1], Prediction: 2.0005525543652376\n",
            "Input: [2], Prediction: 4.00041994131758\n",
            "Input: [3], Prediction: 6.000287328269923\n",
            "Input: [4], Prediction: 8.000154715222267\n",
            "Predicciones: [2.00055255 4.00041994 6.00028733 8.00015472]\n"
          ]
        }
      ]
    }
  ]
}