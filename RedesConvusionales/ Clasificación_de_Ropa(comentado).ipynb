{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMGcnhtCwgTDV8Dk6ieW8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnaKarenDRIV/FinanzasUniversitarias/blob/main/RedesConvusionales/%20Clasificaci%C3%B3n_de_Ropa(comentado).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIFuIsw4qf67"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Nota: ejecutar en un entorno con TensorFlow instalado (GPU opcional).\n",
        "\n",
        "import numpy as np  # manejo de arreglos y operaciones numéricas\n",
        "import tensorflow as tf  # framework de deep learning\n",
        "from tensorflow.keras.models import Sequential  # modelo secuencial (capas apiladas)\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Softmax\n",
        "from tensorflow.keras.optimizers import Adam  # optimizador Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist  # dataset Fashion MNIST\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy  # pérdida para etiquetas enteras\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # evaluación: matriz de confusión\n",
        "import seaborn as sns  # visualización estadística (opcional)\n",
        "import matplotlib.pyplot as plt  # visualizaciones con matplotlib\n",
        "\n",
        "# -------------------------\n",
        "# 1) Información y carga del dataset\n",
        "# -------------------------\n",
        "# El dataset Fashion MNIST ya viene dividido en entrenamiento y prueba por defecto.\n",
        "# Cada imagen es de 28x28 píxeles en escala de grises.\n",
        "\n",
        "# Referencia: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data\n",
        "\n",
        "# Definimos nombres de clases (el índice corresponde a la etiqueta numérica)\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Cargar los datos (devuelve tuplas para train y test)\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Exploración rápida / visualización\n",
        "# -------------------------\n",
        "# Mostrar una imagen de ejemplo y su etiqueta para verificar que los datos se cargaron bien.\n",
        "index = 10  # índice de ejemplo a visualizar\n",
        "image = train_images[index]\n",
        "label = train_labels[index]\n",
        "\n",
        "# Imprimir la matriz numérica (28x28) que representa la imagen\n",
        "print(image)\n",
        "\n",
        "# Mostrar la imagen con matplotlib (mapa de grises)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(image, cmap=plt.cm.gray)\n",
        "plt.title(f'Etiqueta número: {label} -> {class_names[label]}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 3) Preprocesamiento\n",
        "# -------------------------\n",
        "# Revisar formas de los arreglos\n",
        "print('Forma train_images:', train_images.shape)  # (60000, 28, 28)\n",
        "print('Forma test_images: ', test_images.shape)   # (10000, 28, 28)\n",
        "\n",
        "# Normalizar los valores de píxel a [0, 1] para estabilizar el entrenamiento\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Keras Conv2D espera entradas con forma (alto, ancho, canales). Actualmente las imágenes son (28,28).\n",
        "# Agregamos la dimensión de canales = 1 para indicar que son en escala de grises.\n",
        "train_images = np.expand_dims(train_images, -1)  # ahora (60000, 28, 28, 1)\n",
        "test_images = np.expand_dims(test_images, -1)    # ahora (10000, 28, 28, 1)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Definición del modelo (CNN)\n",
        "# -------------------------\n",
        "# Construimos un modelo secuencial con varias capas convolucionales y capas densas al final.\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # 1a conv: 32 filtros 3x3\n",
        "    MaxPooling2D((2, 2)),  # primer max-pool reduce la resolución espacial a la mitad\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # 2a conv: 64 filtros\n",
        "    MaxPooling2D((2, 2)),  # segundo max-pool\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # 3a conv: 64 filtros\n",
        "    Flatten(),  # aplana los mapas de características a un vector\n",
        "    Dense(64, activation='relu'),  # capa totalmente conectada intermedia\n",
        "    Dense(10)  # capa de salida: 10 unidades (logits). No aplicamos softmax aquí.\n",
        "])\n",
        "\n",
        "# Resumen del modelo para revisar capas y parámetros\n",
        "model.summary()\n",
        "\n",
        "# -------------------------\n",
        "# 5) Compilación\n",
        "# -------------------------\n",
        "# Definimos optimizador Adam con tasa de aprendizaje especificada\n",
        "learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Compilar el modelo: indicamos optimizador, función de pérdida (from_logits=True porque no usamos softmax)\n",
        "model.compile(optimizer=adam_optimizer,\n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# 6) Entrenamiento\n",
        "# -------------------------\n",
        "# Ajustar (entrenar) el modelo con los datos de entrenamiento y validar en el conjunto de prueba\n",
        "# Nota: se puede ajustar batch_size, callbacks (EarlyStopping, ModelCheckpoint), etc.\n",
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "# -------------------------\n",
        "# 7) Evaluación final\n",
        "# -------------------------\n",
        "# Evaluar el modelo en el conjunto de prueba para obtener pérdida y precisión\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f'Pérdida en test: {test_loss:.4f} - Precisión en test: {test_acc:.4f}')\n",
        "\n",
        "# -------------------------\n",
        "# 8) Predicciones y matriz de confusión\n",
        "# -------------------------\n",
        "# Construimos un modelo que incluye softmax para convertir logits en probabilidades\n",
        "probability_model = Sequential([model, Softmax()])\n",
        "\n",
        "# Obtener predicciones (probabilidades) para todas las imágenes de test\n",
        "predictions = probability_model.predict(test_images)\n",
        "# Convertir probabilidades a etiquetas predichas (índice de máxima probabilidad)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calcular la matriz de confusión usando sklearn\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# Mostrar la matriz de confusión (gráfico grande para mejor lectura)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(ax=ax, xticks_rotation='vertical')\n",
        "plt.title('Matriz de confusión - Fashion MNIST')\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 9) BONUS: función para graficar imágenes con predicción\n",
        "# -------------------------\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    \"\"\"Dibuja la imagen i junto con la etiqueta predicha y la verdadera.\n",
        "    predictions_array: vector de probabilidades (para una sola imagen).\n",
        "    true_label: arreglo de etiquetas verdaderas.\n",
        "    img: arreglo de imágenes (sin normalizar o ya normalizado está bien).\n",
        "    \"\"\"\n",
        "    true_label, img = true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    # img puede venir con forma (28,28,1) o (28,28)\n",
        "    plt.imshow(img.reshape((28, 28)), cmap=plt.cm.binary)\n",
        "\n",
        "    # índice de la clase predicha\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    # color azul si acierta, rojo si falla\n",
        "    color = 'blue' if predicted_label == true_label else 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(\n",
        "        class_names[predicted_label],\n",
        "        100 * np.max(predictions_array),\n",
        "        class_names[true_label]\n",
        "    ), color=color)\n",
        "\n",
        "# Graficar varias predicciones de ejemplo\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows * num_cols\n",
        "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
        "    plot_image(i, predictions[i], test_labels, test_images)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}