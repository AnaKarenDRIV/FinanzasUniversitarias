{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORiBmGFSCMABJwEgBKVA7l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnaKarenDRIV/FinanzasUniversitarias/blob/main/RedesConvusionales/Clasificaci%C3%B3n_de_d%C3%ADgitos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsoU1PUBrfZE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------\n",
        "#  Importaciones\n",
        "# -------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, recall_score\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# -------------------------\n",
        "# 1) Cargar y explorar el dataset\n",
        "# -------------------------\n",
        "# Cargamos el dataset \"digits\" (8x8 píxeles, 10 clases: dígitos 0-9)\n",
        "digits = load_digits()\n",
        "\n",
        "# Las claves del diccionario nos indican qué contiene el dataset\n",
        "print(digits.keys())\n",
        "# 'data'  : arreglos 2D plano (n_samples, 64)\n",
        "# 'images': arreglos 3D con forma (n_samples, 8, 8)\n",
        "# 'target' : etiquetas enteras\n",
        "\n",
        "# Imprimir descripción breve del dataset\n",
        "print(digits.DESCR[:400], \"...\\n\")  # muestra las primeras 400 columnas de la descripción\n",
        "\n",
        "# Visualizar una imagen de ejemplo\n",
        "index = 0\n",
        "image = digits.images[index]\n",
        "label = digits.target[index]\n",
        "print('Matriz de la imagen (8x8):\\n', image)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(image, cmap=plt.cm.gray)\n",
        "plt.title(f'Dígito: {label}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 2) División en entrenamiento y prueba + one-hot encode\n",
        "# -------------------------\n",
        "# Los modelos Keras para clasificación multiclase a menudo esperan etiquetas en formato one-hot\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    digits.data, digits.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convertir etiquetas a one-hot (10 clases)\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print('Forma X_train:', X_train.shape)\n",
        "print('Forma X_test:', X_test.shape)\n",
        "\n",
        "# Mostrar cómo se ve el primer elemento antes de escalado (al aplanarlo a 8x8)\n",
        "reshaped_tensor = tf.reshape(X_train[0], shape=(8, 8))\n",
        "print('Primer elemento (reshape 8x8) antes de escalado:\\n', reshaped_tensor.numpy())\n",
        "\n",
        "# -------------------------\n",
        "# 3) Escalado / normalización\n",
        "# -------------------------\n",
        "# Elegimos StandardScaler para centrar cada característica en 0 con varianza 1.\n",
        "# Importante: usamos el mismo scaler (fit sobre X_train) luego para transformar nuevas imágenes.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Ver cómo se ve el primer elemento tras la normalización (redondeado para mostrar)\n",
        "reshaped_tensor = tf.reshape(X_train_scaled[0], shape=(8, 8))\n",
        "reshaped_tensor = tf.floor(reshaped_tensor * 100) / 100\n",
        "print('Primer elemento (reshape 8x8) tras StandardScaler (redondeado):\\n', reshaped_tensor.numpy())\n",
        "\n",
        "# Para la CNN necesitamos la forma (n_samples, 8, 8, 1)\n",
        "X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], 8, 8, 1)).astype('float32')\n",
        "X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], 8, 8, 1)).astype('float32')\n",
        "\n",
        "# -------------------------\n",
        "# 4) Definición del modelo CNN\n",
        "# -------------------------\n",
        "# Nota: las imágenes son pequeñas (8x8) por lo que la arquitectura debe ser sencilla para no sobreajustar.\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # salida con softmax -> probabilidades sobre 10 clases\n",
        "])\n",
        "\n",
        "# Mostrar resumen de la arquitectura\n",
        "model.summary()\n",
        "\n",
        "# -------------------------\n",
        "# 5) Compilación\n",
        "# -------------------------\n",
        "# Usamos Adam y entropía cruzada categórica (one-hot)\n",
        "learning_rate = 0.001\n",
        "adam_optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# 6) Entrenamiento\n",
        "# -------------------------\n",
        "# Entrenamos el modelo; validation_split separa parte del X_train para validación interna.\n",
        "history = model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# -------------------------\n",
        "# 7) Visualizar la curva de pérdida\n",
        "# -------------------------\n",
        "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "plt.title('Función de pérdida durante el entrenamiento')\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# 8) Evaluación en conjunto de prueba\n",
        "# -------------------------\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# -------------------------\n",
        "# 9) Matriz de confusión y sensibilidad (recall) por clase\n",
        "# -------------------------\n",
        "# Obtener predicciones y convertir de probabilidades a clases\n",
        "y_pred = model.predict(X_test_cnn)\n",
        "# clases predichas (0..9)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# convertir y_test (one-hot) a etiquetas enteras\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "sensitivity = recall_score(y_test_classes, y_pred_classes, average=None)\n",
        "\n",
        "# Visualizar matriz de confusión\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar sensibilidad por clase\n",
        "print('Sensitivity (Recall) for each class:')\n",
        "for i in range(10):\n",
        "    print(f'Class {i}: {sensitivity[i]:.4f}')\n",
        "\n",
        "# -------------------------\n",
        "# 10) BONUS: Predecir una imagen externa (mi_numero.png)\n",
        "# -------------------------\n",
        "# Para predecir una imagen externa debemos aplicar exactamente el mismo preprocesamiento usado\n",
        "# en X_train: convertir a escala de grises, redimensionar a 8x8, reescalar los valores al mismo rango\n",
        "# y aplicar el mismo scaler (fit solo sobre X_train).\n",
        "\n",
        "# Ejemplo de lectura y preprocesamiento:\n",
        "ruta = 'mi_numero.png'  # modificar según ubicación real\n",
        "i = Image.open(ruta).convert('L')  # L -> escala de grises\n",
        "\n",
        "# Redimensionar a 8x8 (no mantener proporción si queremos exactamente 8x8)\n",
        "i_resized = i.resize((8, 8), Image.Resampling.LANCZOS)\n",
        "\n",
        "# Convertir a array y escalar para coincidir con los valores originales (si se desea 0-16 como en la demo):\n",
        "# Si el dataset original estuviera en 0-16, habría que transformar accordingly. En nuestro pipeline usamos\n",
        "# StandardScaler sobre los datos planos (ya extraídos con digits.data), así que debemos aplicar la misma\n",
        "# transformación estadística: primero aplanar, luego aplicar scaler.transform, y dar forma (1,8,8,1).\n",
        "\n",
        "img_array = np.array(i_resized).astype('float32')\n",
        "# Si los datos originales estaban en 0-16 (dataset 'digits' lo presentaba así visualmente), se puede mapear.\n",
        "# Pero dado que usamos StandardScaler fit sobre X_train (en escala original), aplicamos la misma transformación:\n",
        "img_flat = img_array.reshape(1, -1)  # forma (1, 64)\n",
        "# IMPORTANTE: usamos el mismo 'scaler' definido y fit sobre X_train\n",
        "img_scaled = scaler.transform(img_flat)  # (1, 64)\n",
        "img_cnn = img_scaled.reshape((1, 8, 8, 1)).astype('float32')\n",
        "\n",
        "# Predecir\n",
        "pred = model.predict(img_cnn)\n",
        "digit = np.argmax(pred, axis=1)[0]\n",
        "print('Predicción:', digit)\n",
        "\n",
        "# Fin del script. Guardar modelos, usar callbacks (EarlyStopping, ModelCheckpoint) o aumentar data augmentation\n",
        "# pueden mejorar rendimiento en datasets pequeños o cuando hay sobreajuste.\n"
      ]
    }
  ]
}